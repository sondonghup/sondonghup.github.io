---
# multilingual page pair id, this must pair with translations of this page. (This name must be unique)
lng_pair: id_Examples
title: seq2seq

# post specific
# if not specified, .name will be used from _data/owner/[language].yml
author: Mr. Green's Workshop
# multiple category is not supported
category: jekyll
# multiple tag entries are possible
tags: [jekyll, sample, example post]
# thumbnail image for post
img: ":post_pic1.jpg"
# disable comments on this page
#comments_disable: true

# publish date
date: 2022-08-16 20:30:06 +0900

# seo
# if not specified, date will be used.
#meta_modify_date: 2022-08-16 20:30:06 +0900
# check the meta_common_description in _data/owner/[language].yml
#meta_description: ""

# optional
# if you enabled image_viewer_posts you don't need to enable this. This is only if image_viewer_posts = false
#image_viewer_on: true
# if you enabled image_lazy_loader_posts you don't need to enable this. This is only if image_lazy_loader_posts = false
#image_lazy_loader_on: true
# exclude from on site search
#on_site_search_exclude: true
# exclude from search engines
#search_engine_exclude: true
# to disable this page, simply set published: false or delete this file
#published: false
---

<!-- outline-start -->

This is an example page to display markdown related styles for Mr. Green Jekyll Theme.

<!-- outline-end -->

### seq2seq
{:data-align="center"}
seq2seq는 encoder-decoder 모델이라고도 한다.  
  
  
  
### 초창기 기계 번역
![image](https://user-images.githubusercontent.com/42092560/184919250-5eaafb53-b4c1-43bc-aee0-0316eb6bb613.png)  

초창기 기계 번역은 rnn을 하나만 사용하였다.  
  
이는 입력과 출력의 토큰개수가 같다고 가정해야 하는 문제점과 한국어와 영어 같이 어순이 다른경우  
  
좋은 결과를 낼 수 없었다.  
  
이를 해결 하기 위해 rnn을 두개를 사용하여 하나는 encoder, 하나는 decoder로 사용하도록 한다.  
  
### seq2seq의 구성
  
![image](https://user-images.githubusercontent.com/42092560/184922496-8307fe14-cd9a-466d-949e-fffdf01ebee5.png)  
  
위의 문제를 해결하기 위해 인코더에서 문맥 벡터를 추출한다.  
  
이후 문맥 벡터로부터 디코더가 번역 결과를 추론한다.  
  
이때 문맥 벡터는 인코더의 마지막 은닉계층의 벡터를 사용한다.  
  
디코더에서 고정된 크기의 문맥 벡터를 받아서 은닉계층의 벡터를 뽑아낸 뒤  
  
간단한 affine layer를 거쳐서 값을 출력한다.  
  
번역의 경우 나라마다의 문법때문에 입력개수와 출력개수가 다를 수 있다.  
  
근데 고정된 크기의 문맥 벡터라면 만약 학습을 할때 짧은 문장만 학습하다가  
  
테스트에 갑자기 긴 문장이 나오는 경우  
  
벡터가 문맥 정보를 다담지 못하여 성능저하가 일어나지 않을까 생각이 든다.  
  
※ 번역같은 경우 입력길이가 다른 경우가 있다.  
  
이때 미니배치처리를 하기위해서 입력길이를 같게 해줘야 하는데 이때 padding을 사용한다.  
  
padding은 입력길이를 같게 하기 위해 뒤에 의미없는 값을 넣어 주는 것을 말한다.  
ex)  
![image](https://user-images.githubusercontent.com/42092560/184928557-6c00c20e-e0a7-47d4-bf39-3d835564c5dc.png)  
  
위와 같이 가장 긴 길이의 문장에 맞춰 0 값을 넣어준다.  
  
이때 seq2seq에서 의미없는 값도 처리하기 때문에 다음 처리를 해준다.  
  
● decoder에서 나온 출력값은 0이 포함된 입력의 값이기 때문에 mask 처리를 해준다.  
  
mask는 1,0으로 이루어진 벡터로 위의 사진 같은 경우  
  
![image](https://user-images.githubusercontent.com/42092560/184930461-ce5b2211-cbab-4990-b24e-b5f521104642.png)
  
이 값을 곱해준다 이렇게 되면 기존에 0인 곳은 0을 곱해 0으로 만들어 버리고  
  
유의미한 부분의 값만 남게 된다.  
  
● encoder에서 입력값이 0 (padding)이면 이전의 히든계층을 넘기는 방식으로 처리한다.  
  
### seq2seq 개선  
  
● encoder에 입력되는 값의 순서를 바꾸는 것  
  
보통 먼저 들어간 단어는 뒤로 갈수록 정보가 덜 담기게 되는데 순서를 바꿔주게 되면  
  
앞에 단어의 정보가 많이 살아간다.  
  
그러면 어차피 뒤에 있는 단어가 앞에 들어가니 똑같은거 아닌가 생각이 들기도 한다.  
  
우리나라 말이나 영어나 생각해보면 가장 중요한 주어 동사 목적어 등등이 앞에 위치하고  
  
꾸며주는 말들이 뒤로 가는게 일반적이라 꾸며주는 말 같이 덜 중요한 것을 미리 넣고  
  
주체가 되는 말들을 뒤에 넣어 문맥을 더 살리는 느낌인것 같다.  
  
● 문맥 벡터를 공유하는 것  
  
원래는 문맥 정보가 들어간 벡터를 맨 처음 decoder에서만 받는다.  

근데 중요한 문맥 정보면 공유하는게 더 좋을테니 이런 방식을 채택한것이다.  
  
첫 rnn이 아니라 매번 문맥정보를 넣어주고 매번 affine layer에도 넣어준다.  
  
이때 벡터는 두개가 입력되는것이 아닌 두 벡터를 concat해서 넣어준다.  
  
  
### seq2seq의 활용  
  
● 챗봇  
  
기존에 seq2seq는 번역에 쓴다고 하였는데 챗봇 같은 경우 봇과 유저의 대화이므로  
  
영 한 번역을 예를들면 봇의대화가 영어 유저의 대화가 번역된 한글로 생각하면 된다.  
  
● 이미지 캡셔닝  
  
기존의 부분에서 encoder를 cnn으로 바꿔준 구조이다.
  
![image](https://user-images.githubusercontent.com/42092560/185382125-f24f56b0-49d1-456a-a58f-5c47df832913.png)  
  
cnn의 output은 3차원 이므로 affine을 통해 1차원으로 변경한뒤 decoder에 입력해준다.  
  


